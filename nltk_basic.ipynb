{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phân tích đoạn văn bản bằng nltk.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy = True\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.text import Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi', '.', 'my', 'name']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_str = \"Hi. My name is Chung. I am 24 years old. I have studied Computer Science at Hanoi University of Science and Technology.\"\n",
    "tokens = word_tokenize(my_str)               # Chuyen xau thanh cac tokens\n",
    "tokens = [word.lower() for word in tokens]   # Chuyen cac tokens ve dang chu thuong\n",
    "tokens[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Text: hi . my name is chung . i...>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = Text(tokens)                             # Chuyen day cac tokens ve thanh mot cau                               \n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no matches\n"
     ]
    }
   ],
   "source": [
    "t.concordance('our')                         # Xac dinh ngu canh cua mot tu nao day trong cau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "t.collocations()                             # Xac dinh cac tu tao thanh mot cum tu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.count('you')                               # Dem so luong mot tu nao day trong cau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'Chung' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-19430f082618>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Chung'\u001b[0m\u001b[1;33m)\u001b[0m                             \u001b[1;31m# Tim chi so cua mot tu nao day trong cau\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\chungphb\\lib\\site-packages\\nltk\\text.py\u001b[0m in \u001b[0;36mindex\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    456\u001b[0m         \u001b[0mFind\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mindex\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfirst\u001b[0m \u001b[0moccurrence\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m         \"\"\"\n\u001b[1;32m--> 458\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    459\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreadability\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: 'Chung' is not in list"
     ]
    }
   ],
   "source": [
    "t.index('Chung')                             # Tim chi so cua mot tu nao day trong cau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.similar('years')                           # Tim tu co cung ngu canh voi mot tu nao day trong cau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.dispersion_plot(['you', 'I', 'we'])        # Vi tri cua mot so tu nhat dinh nao day trong cau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.plot(10)                                   # Tan suat xuat hien cua 10 tu xuat hien nhieu nhat trong cau. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.vocab()                                    # Danh sach cac tu trong cau "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xây dựng n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_str = 'Xin chào. Tôi là Chung. Tôi 24 tuổi. Tôi học Khoa học máy tính tại Trường Đại học Bách Khoa Hà Nội.'\n",
    "my_str = my_str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer('[a-zA-Z0-9àôổọáíạườộđ]+')   # Xay dung tokenizer voi token duoc dinh nghia boi mot regex cho truoc\n",
    "tokens = tokenizer.tokenize(my_str)                      # Xac dinh danh sach cac tokens trong cau\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "four_grams = []\n",
    "for word in tokens:  \n",
    "    # Tach ky tu trong mot token thanh cac bi-grams, su dung ky tu * de chen ben trai va ben phai \n",
    "    four_grams.append(list(ngrams(word, 2, pad_left = True, pad_right = True, left_pad_symbol = '*', right_pad_symbol = '*')))\n",
    "four_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "four_grams = [word for sublist in four_grams for word in sublist]\n",
    "four_grams[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_four_grams = four_grams\n",
    "for i, val in enumerate(four_grams):\n",
    "    list_four_grams[i] = ''.join(val)\n",
    "list_four_grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phát hiện ngôn ngữ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phương thức 1. Đếm số lượng stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.readme().replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.fileids()                                      # Danh sach cac file, moi file tuong ung voi mot tap cac stop words trong ngon ngu tuong ung "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.raw('english').replace('\\n', ' ')              # Noi dung file english chua cac stop words trong Tieng Anh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.words('spanish')[:16]                          # Danh sach 16 stop words dau tien trong tieng Tay Ban Nha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "text = \"Yo no te puedo dar el dinero hoy, pero te lo voy a poder dar mañana.\"\n",
    "tokens = wordpunct_tokenize(text)\n",
    "words = [word.lower() for word in tokens]\n",
    "words_set = set(words)                                   # Su dung set de ap dung ham intersection\n",
    "\n",
    "language_ratios = {}\n",
    "for language in stopwords.fileids():\n",
    "    stopwords_set = set(stopwords.words(language))\n",
    "    common_elements = words_set.intersection(stopwords_set)\n",
    "    language_ratios[language] = len(common_elements)     # Dem so luong stop words ung voi moi ngon ngu trong cau da cho\n",
    "\n",
    "language_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_language = max(language_ratios, key = language_ratios.get)\n",
    "predicted_language"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:chungphb]",
   "language": "python",
   "name": "conda-env-chungphb-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
