{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phân tích đoạn văn bản bằng nltk.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy = True\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.text import Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_str = \"Hi. My name is Chung. I am 24 years old. I have studied Computer Science at Hanoi University of Science and Technology.\"\n",
    "tokens = word_tokenize(my_str)               # Chuyen xau thanh cac tokens\n",
    "tokens = [word.lower() for word in tokens]   # Chuyen cac tokens ve dang chu thuong\n",
    "tokens[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Text(tokens)                             # Chuyen day cac tokens ve thanh mot cau                               \n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.concordance('our')                         # Xac dinh ngu canh cua mot tu nao day trong cau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.collocations()                             # Xac dinh cac tu tao thanh mot cum tu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.count('you')                               # Dem so luong mot tu nao day trong cau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.index('up')                                # Tim chi so cua mot tu nao day trong cau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.similar('years')                           # Tim tu co cung ngu canh voi mot tu nao day trong cau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.dispersion_plot(['you', 'I', 'we'])        # Vi tri cua mot so tu nhat dinh nao day trong cau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.plot(10)                                   # Tan suat xuat hien cua 10 tu xuat hien nhieu nhat trong cau. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.vocab()                                    # Danh sach cac tu trong cau "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_str = 'Xin chào. Tôi là Chung. Tôi 24 tuổi. Tôi học Khoa học máy tính tại Trường Đại học Bách Khoa Hà Nội.'\n",
    "my_str = my_str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xin',\n",
       " 'chào',\n",
       " 'tôi',\n",
       " 'là',\n",
       " 'chung',\n",
       " 'tôi',\n",
       " '24',\n",
       " 'tuổi',\n",
       " 'tôi',\n",
       " 'học',\n",
       " 'khoa',\n",
       " 'học',\n",
       " 'máy',\n",
       " 'tính',\n",
       " 'tại',\n",
       " 'trường',\n",
       " 'đại',\n",
       " 'học',\n",
       " 'bách',\n",
       " 'khoa',\n",
       " 'hà',\n",
       " 'nội']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer('[a-zA-Z0-9àôổọáíạườộđ]+')   # Xay dung tokenizer voi token duoc dinh nghia boi mot regex cho truoc\n",
    "tokens = tokenizer.tokenize(my_str)                      # Xac dinh danh sach cac tokens trong cau\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('*', 'x'), ('x', 'i'), ('i', 'n'), ('n', '*')],\n",
       " [('*', 'c'), ('c', 'h'), ('h', 'à'), ('à', 'o'), ('o', '*')],\n",
       " [('*', 't'), ('t', 'ô'), ('ô', 'i'), ('i', '*')],\n",
       " [('*', 'l'), ('l', 'à'), ('à', '*')],\n",
       " [('*', 'c'), ('c', 'h'), ('h', 'u'), ('u', 'n'), ('n', 'g'), ('g', '*')],\n",
       " [('*', 't'), ('t', 'ô'), ('ô', 'i'), ('i', '*')],\n",
       " [('*', '2'), ('2', '4'), ('4', '*')],\n",
       " [('*', 't'), ('t', 'u'), ('u', 'ổ'), ('ổ', 'i'), ('i', '*')],\n",
       " [('*', 't'), ('t', 'ô'), ('ô', 'i'), ('i', '*')],\n",
       " [('*', 'h'), ('h', 'ọ'), ('ọ', 'c'), ('c', '*')],\n",
       " [('*', 'k'), ('k', 'h'), ('h', 'o'), ('o', 'a'), ('a', '*')],\n",
       " [('*', 'h'), ('h', 'ọ'), ('ọ', 'c'), ('c', '*')],\n",
       " [('*', 'm'), ('m', 'á'), ('á', 'y'), ('y', '*')],\n",
       " [('*', 't'), ('t', 'í'), ('í', 'n'), ('n', 'h'), ('h', '*')],\n",
       " [('*', 't'), ('t', 'ạ'), ('ạ', 'i'), ('i', '*')],\n",
       " [('*', 't'),\n",
       "  ('t', 'r'),\n",
       "  ('r', 'ư'),\n",
       "  ('ư', 'ờ'),\n",
       "  ('ờ', 'n'),\n",
       "  ('n', 'g'),\n",
       "  ('g', '*')],\n",
       " [('*', 'đ'), ('đ', 'ạ'), ('ạ', 'i'), ('i', '*')],\n",
       " [('*', 'h'), ('h', 'ọ'), ('ọ', 'c'), ('c', '*')],\n",
       " [('*', 'b'), ('b', 'á'), ('á', 'c'), ('c', 'h'), ('h', '*')],\n",
       " [('*', 'k'), ('k', 'h'), ('h', 'o'), ('o', 'a'), ('a', '*')],\n",
       " [('*', 'h'), ('h', 'à'), ('à', '*')],\n",
       " [('*', 'n'), ('n', 'ộ'), ('ộ', 'i'), ('i', '*')]]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "four_grams = []\n",
    "for word in tokens:  \n",
    "    # Tach ky tu trong mot token thanh cac bi-grams, su dung ky tu * de chen ben trai va ben phai \n",
    "    four_grams.append(list(ngrams(word, 2, pad_left = True, pad_right = True, left_pad_symbol = '*', right_pad_symbol = '*')))\n",
    "four_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('*', 'x'),\n",
       " ('x', 'i'),\n",
       " ('i', 'n'),\n",
       " ('n', '*'),\n",
       " ('*', 'c'),\n",
       " ('c', 'h'),\n",
       " ('h', 'à'),\n",
       " ('à', 'o'),\n",
       " ('o', '*'),\n",
       " ('*', 't')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "four_grams = [word for sublist in four_grams for word in sublist]\n",
    "four_grams[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['*x',\n",
       " 'xi',\n",
       " 'in',\n",
       " 'n*',\n",
       " '*c',\n",
       " 'ch',\n",
       " 'hà',\n",
       " 'ào',\n",
       " 'o*',\n",
       " '*t',\n",
       " 'tô',\n",
       " 'ôi',\n",
       " 'i*',\n",
       " '*l',\n",
       " 'là',\n",
       " 'à*',\n",
       " '*c',\n",
       " 'ch',\n",
       " 'hu',\n",
       " 'un',\n",
       " 'ng',\n",
       " 'g*',\n",
       " '*t',\n",
       " 'tô',\n",
       " 'ôi',\n",
       " 'i*',\n",
       " '*2',\n",
       " '24',\n",
       " '4*',\n",
       " '*t',\n",
       " 'tu',\n",
       " 'uổ',\n",
       " 'ổi',\n",
       " 'i*',\n",
       " '*t',\n",
       " 'tô',\n",
       " 'ôi',\n",
       " 'i*',\n",
       " '*h',\n",
       " 'họ',\n",
       " 'ọc',\n",
       " 'c*',\n",
       " '*k',\n",
       " 'kh',\n",
       " 'ho',\n",
       " 'oa',\n",
       " 'a*',\n",
       " '*h',\n",
       " 'họ',\n",
       " 'ọc',\n",
       " 'c*',\n",
       " '*m',\n",
       " 'má',\n",
       " 'áy',\n",
       " 'y*',\n",
       " '*t',\n",
       " 'tí',\n",
       " 'ín',\n",
       " 'nh',\n",
       " 'h*',\n",
       " '*t',\n",
       " 'tạ',\n",
       " 'ại',\n",
       " 'i*',\n",
       " '*t',\n",
       " 'tr',\n",
       " 'rư',\n",
       " 'ườ',\n",
       " 'ờn',\n",
       " 'ng',\n",
       " 'g*',\n",
       " '*đ',\n",
       " 'đạ',\n",
       " 'ại',\n",
       " 'i*',\n",
       " '*h',\n",
       " 'họ',\n",
       " 'ọc',\n",
       " 'c*',\n",
       " '*b',\n",
       " 'bá',\n",
       " 'ác',\n",
       " 'ch',\n",
       " 'h*',\n",
       " '*k',\n",
       " 'kh',\n",
       " 'ho',\n",
       " 'oa',\n",
       " 'a*',\n",
       " '*h',\n",
       " 'hà',\n",
       " 'à*',\n",
       " '*n',\n",
       " 'nộ',\n",
       " 'ội',\n",
       " 'i*']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_four_grams = four_grams\n",
    "for i, val in enumerate(four_grams):\n",
    "    list_four_grams[i] = ''.join(val)\n",
    "list_four_grams"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:chungphb]",
   "language": "python",
   "name": "conda-env-chungphb-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
